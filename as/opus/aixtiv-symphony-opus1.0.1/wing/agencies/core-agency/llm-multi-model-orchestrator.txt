                "cache": {
                    "enabled": True,
                    "ttl": 86400,  # 24 hours
                    "firebase": {
                        "enabled": True,
                        "collection": "llm_cache",
                        "project_id": "your-project-id",
                        "service_account_file": "firebase_service_account.json"
                    },
                    "local": {
                        "enabled": True,
                        "max_entries": 1000
                    }
                },
                "governance": {
                    "enabled": True,
                    "audit_trail": True,
                    "content_filtering": True,
                    "provider_metrics": True
                },
                "cost_management": {
                    "enabled": True,
                    "tracking": True,
                    "budget_limits": {
                        "daily": 10.0,
                        "monthly": 200.0
                    },
                    "token_allocation": {
                        "openai": 0.6,
                        "anthropic": 0.3,
                        "google": 0.1
                    }
                }
            }
    
    def _initialize_clients(self) -> None:
        """Initialize API clients for each enabled provider."""
        # Initialize OpenAI
        if self.config["providers"]["openai"]["enabled"]:
            try:
                api_key = self.config["providers"]["openai"]["api_key"] or os.environ.get("OPENAI_API_KEY")
                if not api_key:
                    self.logger.warning("OpenAI API key not provided. OpenAI provider will be disabled.")
                else:
                    self.clients[ModelProvider.OPENAI.value] = OpenAI(
                        api_key=api_key,
                        timeout=self.config["providers"]["openai"].get("timeout", 60),
                        max_retries=self.config["providers"]["openai"].get("max_retries", 3)
                    )
                    self.logger.info("OpenAI client initialized")
            except Exception as e:
                self.logger.error(f"Failed to initialize OpenAI client: {str(e)}")
        
        # Initialize Anthropic
        if self.config["providers"]["anthropic"]["enabled"]:
            try:
                api_key = self.config["providers"]["anthropic"]["api_key"] or os.environ.get("ANTHROPIC_API_KEY")
                if not api_key:
                    self.logger.warning("Anthropic API key not provided. Anthropic provider will be disabled.")
                else:
                    self.clients[ModelProvider.ANTHROPIC.value] = Anthropic(
                        api_key=api_key
                    )
                    self.logger.info("Anthropic client initialized")
            except Exception as e:
                self.logger.error(f"Failed to initialize Anthropic client: {str(e)}")
        
        # Initialize Google (placeholder)
        if self.config["providers"]["google"]["enabled"]:
            try:
                api_key = self.config["providers"]["google"]["api_key"] or os.environ.get("GOOGLE_API_KEY")
                if not api_key:
                    self.logger.warning("Google API key not provided. Google provider will be disabled.")
                else:
                    # This is a placeholder for actual Google client initialization
                    self.clients[ModelProvider.GOOGLE.value] = {"api_key": api_key}
                    self.logger.info("Google client placeholder initialized")
            except Exception as e:
                self.logger.error(f"Failed to initialize Google client: {str(e)}")
        
        # Initialize Azure OpenAI (placeholder)
        if self.config["providers"]["azure_openai"]["enabled"]:
            try:
                api_key = self.config["providers"]["azure_openai"]["api_key"] or os.environ.get("AZURE_OPENAI_API_KEY")
                endpoint = self.config["providers"]["azure_openai"]["endpoint"] or os.environ.get("AZURE_OPENAI_ENDPOINT")
                if not api_key or not endpoint:
                    self.logger.warning("Azure OpenAI credentials not provided. Azure OpenAI provider will be disabled.")
                else:
                    # This is a placeholder for actual Azure OpenAI client initialization
                    self.clients[ModelProvider.AZURE_OPENAI.value] = {"api_key": api_key, "endpoint": endpoint}
                    self.logger.info("Azure OpenAI client placeholder initialized")
            except Exception as e:
                self.logger.error(f"Failed to initialize Azure OpenAI client: {str(e)}")
    
    def _initialize_cache(self) -> None:
        """Initialize the cache system."""
        try:
            # Initialize Firebase cache if enabled
            if self.config["cache"]["firebase"]["enabled"]:
                cred_path = self.config["cache"]["firebase"]["service_account_file"]
                if not os.path.exists(cred_path):
                    self.logger.warning(f"Firebase credentials file not found: {cred_path}")
                else:
                    try:
                        # Check if Firebase app is already initialized
                        firebase_admin.get_app()
                    except ValueError:
                        # Initialize Firebase app
                        cred = credentials.Certificate(cred_path)
                        firebase_admin.initialize_app(cred, {
                            'projectId': self.config["cache"]["firebase"]["project_id"],
                        })
                    
                    self.db = firestore.client()
                    self.logger.info("Firebase cache initialized")
            
            # Initialize local cache
            if self.config["cache"]["local"]["enabled"]:
                self.cache_manager = LRUCache(self.config["cache"]["local"]["max_entries"])
                self.logger.info("Local cache initialized")
        
        except Exception as e:
            self.logger.error(f"Failed to initialize cache: {str(e)}")
    
    def _route_to_provider(self, prompt: str) -> tuple[str, str]:
        """
        Route the prompt to the appropriate provider based on content.
        
        Args:
            prompt: User prompt
            
        Returns:
            Tuple of (provider, model)
        """
        default_provider = self.config["orchestration"]["default_provider"]
        default_model = self.config["providers"][default_provider]["default_model"]
        
        # Check if content routing is enabled
        if not self.config["orchestration"]["content_routing"]["enabled"]:
            return default_provider, default_model
        
        # Check routing rules
        for rule in self.config["orchestration"]["content_routing"]["rules"]:
            import re
            if re.search(rule["pattern"], prompt, re.IGNORECASE):
                return rule["provider"], rule["model"]
        
        return default_provider, default_model
    
    async def _get_from_cache(self, prompt: str) -> Optional[Dict]:
        """
        Get a response from cache.
        
        Args:
            prompt: User prompt
            
        Returns:
            Cached response or None if not found
        """
        if not self.config["cache"]["enabled"]:
            return None
        
        cache_key = self._generate_cache_key(prompt)
        
        # Check local cache first
        if self.cache_manager:
            cached_response = self.cache_manager.get(cache_key)
            if cached_response:
                self.logger.debug(f"Cache hit (local): {cache_key}")
                return cached_response
        
        # Check Firebase cache
        if self.db:
            try:
                cache_collection = self.config["cache"]["firebase"]["collection"]
                doc_ref = self.db.collection(cache_collection).document(cache_key)
                doc = doc_ref.get()
                
                if doc.exists:
                    data = doc.to_dict()
                    expiry = data.get('expiry', 0)
                    
                    if expiry > time.time():
                        self.logger.debug(f"Cache hit (Firebase): {cache_key}")
                        
                        # Update local cache
                        if self.cache_manager:
                            self.cache_manager.put(cache_key, data['response'])
                        
                        return data['response']
                    else:
                        # Remove expired document
                        doc_ref.delete()
            except Exception as e:
                self.logger.warning(f"Error accessing Firebase cache: {str(e)}")
        
        return None
    
    async def _save_to_cache(self, prompt: str, response: Dict) -> None:
        """
        Save a response to cache.
        
        Args:
            prompt: User prompt
            response: LLM response to cache
        """
        if not self.config["cache"]["enabled"]:
            return
        
        cache_key = self._generate_cache_key(prompt)
        expiry = time.time() + self.config["cache"]["ttl"]
        
        # Update local cache
        if self.cache_manager:
            self.cache_manager.put(cache_key, response)
        
        # Update Firebase cache
        if self.db:
            try:
                cache_collection = self.config["cache"]["firebase"]["collection"]
                cache_data = {
                    'prompt': prompt,
                    'response': response,
                    'expiry': expiry,
                    'created_at': firestore.SERVER_TIMESTAMP,
                    'provider': response.get('provider'),
                    'model': response.get('model')
                }
                
                self.db.collection(cache_collection).document(cache_key).set(cache_data)
                self.logger.debug(f"Cached in Firebase: {cache_key}")
            except Exception as e:
                self.logger.warning(f"Error writing to Firebase cache: {str(e)}")
    
    def _generate_cache_key(self, prompt: str) -> str:
        """
        Generate a unique cache key for a prompt.
        
        Args:
            prompt: The prompt to hash
            
        Returns:
            Cache key
        """
        return hashlib.md5(prompt.encode()).hexdigest()
    
    async def generate_with_openai(self, prompt: str, model: str = None) -> Dict:
        """
        Generate a response using OpenAI.
        
        Args:
            prompt: User prompt
            model: OpenAI model to use
            
        Returns:
            Response dictionary
        """
        client = self.clients.get(ModelProvider.OPENAI.value)
        if not client:
            raise ValueError("OpenAI client not initialized")
        
        if not model:
            model = self.config["providers"]["openai"]["default_model"]
        
        start_time = time.time()
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            
            # Format response
            result = {
                "provider": ModelProvider.OPENAI.value,
                "model": model,
                "content": response.choices[0].message.content,
                "latency": time.time() - start_time,
                "tokens": {
                    "input": response.usage.prompt_tokens,
                    "output": response.usage.completion_tokens,
                    "total": response.usage.total_tokens
                }
            }
            
            return result
        except Exception as e:
            self.logger.error(f"Error generating with OpenAI: {str(e)}")
            raise
    
    async def generate_with_anthropic(self, prompt: str, model: str = None) -> Dict:
        """
        Generate a response using Anthropic.
        
        Args:
            prompt: User prompt
            model: Anthropic model to use
            
        Returns:
            Response dictionary
        """
        client = self.clients.get(ModelProvider.ANTHROPIC.value)
        if not client:
            raise ValueError("Anthropic client not initialized")
        
        if not model:
            model = self.config["providers"]["anthropic"]["default_model"]
        
        start_time = time.time()
        
        try:
            response = client.messages.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1024
            )
            
            # Format response
            result = {
                "provider": ModelProvider.ANTHROPIC.value,
                "model": model,
                "content": response.content[0].text,
                "latency": time.time() - start_time,
                "tokens": {
                    "input": response.usage.input_tokens,
                    "output": response.usage.output_tokens,
                    "total": response.usage.input_tokens + response.usage.output_tokens
                }
            }
            
            return result
        except Exception as e:
            self.logger.error(f"Error generating with Anthropic: {str(e)}")
            raise
    
    async def generate(self, prompt: str, provider: str = None, model: str = None) -> Dict:
        """
        Generate a response using the appropriate provider with fallback support.
        
        Args:
            prompt: User prompt
            provider: Provider to use (optional)
            model: Model to use (optional)
            
        Returns:
            Response dictionary
        """
        # Check cache first
        cached_response = await self._get_from_cache(prompt)
        if cached_response:
            return {**cached_response, "source": "cache"}
        
        # Determine provider and model
        if not provider:
            provider, suggested_model = self._route_to_provider(prompt)
            if not model:
                model = suggested_model
        
        # Set up providers to try (primary + fallbacks)
        providers_to_try = [provider]
        if self.config["orchestration"]["error_fallback"]:
            fallbacks = self.config["orchestration"]["fallback_providers"]
            providers_to_try.extend([fb for fb in fallbacks if fb != provider])
        
        # Try primary provider and fallbacks if needed
        last_error = None
        for current_provider in providers_to_try:
            try:
                if current_provider == ModelProvider.OPENAI.value:
                    response = await self.generate_with_openai(prompt, model if provider == current_provider else None)
                elif current_provider == ModelProvider.ANTHROPIC.value:
                    response = await self.generate_with_anthropic(prompt, model if provider == current_provider else None)
                elif current_provider == ModelProvider.GOOGLE.value:
                    # Placeholder for Google implementation
                    raise NotImplementedError("Google provider not yet implemented")
                elif current_provider == ModelProvider.AZURE_OPENAI.value:
                    # Placeholder for Azure OpenAI implementation
                    raise NotImplementedError("Azure OpenAI provider not yet implemented")
                else:
                    raise ValueError(f"Unknown provider: {current_provider}")
                
                # Add metadata about fallback if applicable
                if current_provider != provider:
                    response["fallback"] = {
                        "original_provider": provider,
                        "reason": str(last_error) if last_error else "unknown"
                    }
                
                # Save to cache
                await self._save_to_cache(prompt, response)
                
                return response
            except Exception as e:
                last_error = e
                self.logger.warning(f"Provider {current_provider} failed: {str(e)}. Trying fallback.")
        
        # If we get here, all providers failed
        raise Exception(f"All providers failed to generate a response. Last error: {str(last_error)}")


class LRUCache:
    """Simple LRU cache implementation for local caching."""
    
    def __init__(self, capacity: int):
        """
        Initialize the LRU cache.
        
        Args:
            capacity: Maximum number of entries to store
        """
        self.capacity = capacity
        self.cache = {}
        self.usage = []
    
    def get(self, key: str) -> Optional[Dict]:
        """
        Get value from cache.
        
        Args:
            key: Cache key
            
        Returns:
            Cached value or None if not found
        """
        if key not in self.cache:
            return None
        
        # Update usage
        self.usage.remove(key)
        self.usage.append(key)
        
        return self.cache[key]
    
    def put(self, key: str, value: Dict) -> None:
        """
        Put value in cache.
        
        Args:
            key: Cache key
            value: Value to cache
        """
        if key in self.cache:
            self.usage.remove(key)
        elif len(self.cache) >= self.capacity:
            # Remove least recently used item
            lru_key = self.usage.pop(0)
            del self.cache[lru_key]
        
        self.cache[key] = value
        self.usage.append(key)


async def main():
    """Main function to demonstrate the orchestrator."""
    parser = argparse.ArgumentParser(description="LLM Multi-Model Orchestrator")
    parser.add_argument("-c", "--config", help="Path to configuration file", default="orchestrator_config.json")
    parser.add_argument("-p", "--prompt", help="Test prompt to generate a response", default="Tell me the benefits of using multiple LLM providers in an enterprise application.")
    args = parser.parse_args()
    
    try:
        # Initialize orchestrator
        orchestrator = ModelOrchestrator(args.config)
        
        # Generate a response
        response = await orchestrator.generate(args.prompt)
        
        # Print response
        print("\n========== LLM Orchestrator Response ==========")
        print(f"Provider: {response['provider']} ({response['model']})")
        print(f"Source: {response.get('source', 'api')}")
        if response.get('fallback'):
            print(f"Fallback from: {response['fallback']['original_provider']} (Reason: {response['fallback']['reason']})")
        print(f"Latency: {response.get('latency', 0):.2f}s")
        
        if response.get('tokens'):
            print(f"Tokens: {response['tokens']['total']} ({response['tokens']['input']} input, {response['tokens']['output']} output)")
        
        print("\n--- Content ---")
        print(response['content'])
        print("=============================================")
        
    except Exception as e:
        logger.error(f"Error in main: {str(e)}")
        return 1
    
    return 0


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())import asyncio
import logging
import os
import json
import time
import argparse
import hashlib
from typing import Dict, List, Optional, Any, Union
from enum import Enum

# Import provider-specific modules
from openai import OpenAI
from anthropic import Anthropic
import firebase_admin
from firebase_admin import credentials, firestore

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("llm-orchestrator")


class ModelProvider(Enum):
    """Enum for supported LLM providers."""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    AZURE_OPENAI = "azure_openai"
    # Add more providers as needed


class ModelOrchestrator:
    """
    Multi-model orchestrator that supports bringing your own LLM.
    Handles routing, fallbacks, and unified caching across providers.
    """
    
    def __init__(self, config_path: str):
        """
        Initialize the model orchestrator.
        
        Args:
            config_path: Path to configuration file
        """
        self.logger = logging.getLogger("ModelOrchestrator")
        self.config = self._load_config(config_path)
        self.clients = {}
        self.cache_manager = None
        self.db = None
        
        # Initialize provider clients
        self._initialize_clients()
        
        # Initialize the cache if enabled
        if self.config.get("cache", {}).get("enabled", False):
            self._initialize_cache()
    
    def _load_config(self, config_path: str) -> Dict:
        """
        Load configuration from file.
        
        Args:
            config_path: Path to the configuration file
            
        Returns:
            Configuration dictionary
        """
        try:
            with open(config_path, "r") as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"Failed to load configuration: {str(e)}")
            # Return default configuration
            return {
                "providers": {
                    "openai": {
                        "enabled": True,
                        "api_key": None,  # Should be set via environment variable
                        "default_model": "gpt-4",
                        "timeout": 60,
                        "max_retries": 3
                    },
                    "anthropic": {
                        "enabled": True,
                        "api_key": None,  # Should be set via environment variable
                        "default_model": "claude-3-sonnet-20240229",
                        "timeout": 60,
                        "max_retries": 3
                    },
                    "google": {
                        "enabled": False,
                        "api_key": None,
                        "default_model": "gemini-pro",
                        "timeout": 60,
                        "max_retries": 3
                    },
                    "azure_openai": {
                        "enabled": False,
                        "api_key": None,
                        "endpoint": None,
                        "default_model": "gpt-4",
                        "deployment_name": "gpt4",
                        "timeout": 60,
                        "max_retries": 3
                    }
                },
                "orchestration": {
                    "default_provider": "openai",
                    "fallback_providers": ["anthropic"],
                    "timeout_fallback": True,
                    "error_fallback": True,
                    "content_routing": {
                        "enabled": True,
                        "rules": [
                            {
                                "pattern": "summarize|summary",
                                "provider": "anthropic",
                                "model": "claude-3-haiku-20240307"
                            },
                            {
                                "pattern": "code|programming|function|algorithm",
                                "provider": "openai",
                                "model": "gpt-4"
                            }
                        ]
                    }
                },
                "cache": {
                    